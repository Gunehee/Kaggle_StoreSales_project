{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gunehee/Kaggle_StoreSales_project/blob/main/Store_Sales_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "258af43a",
      "metadata": {
        "id": "258af43a"
      },
      "source": [
        "# Ecuador Store Sales Forecasting\n",
        "\n",
        "Forecasting product family sales across 54 stores using historical data and external factors."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9178acd",
      "metadata": {
        "id": "c9178acd"
      },
      "source": [
        "## Introduction & Objective"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b1f3e52",
      "metadata": {
        "id": "0b1f3e52"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41694833",
      "metadata": {
        "id": "41694833"
      },
      "outputs": [],
      "source": [
        "#import the necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f21fb344",
      "metadata": {
        "id": "f21fb344"
      },
      "outputs": [],
      "source": [
        "#import the data\n",
        "path = '/content/drive/MyDrive/DS_Army/Store Sales/Store_Sales_Data'\n",
        "\n",
        "data_oil = pd.read_csv(path+'/oil.csv')\n",
        "data_holidays = pd.read_csv(path+'/holidays_events.csv')\n",
        "data_stores = pd.read_csv(path+'/stores.csv')\n",
        "data_train = pd.read_csv(path+'/train.csv')\n",
        "data_transactions = pd.read_csv(path+'/transactions.csv')\n",
        "data_test = pd.read_csv(path+'/test.csv')\n",
        "data_sample_submission = pd.read_csv(path+'/sample_submission.csv')\n",
        "\n",
        "#print out general info abput the datasets\n",
        "names = ['data_oil', 'data_holidays', 'data_stores', 'data_train', 'data_transactions', 'data_test']\n",
        "for name, data in zip(names, [data_oil, data_holidays, data_stores, data_train, data_transactions, data_test]):\n",
        "    print('shape of', name, data.shape)\n",
        "    #print('columns in', name , ':', data.columns )\n",
        "    print ('column types in', name ,':')\n",
        "    print(data.dtypes)\n",
        "    missing_values = data_train[data_train.isnull().any(axis=1)]\n",
        "    if missing_values.empty:\n",
        "        print('There are no missing values')\n",
        "    else:\n",
        "        print(missing_values)\n",
        "    display(data.head())\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e09c4d1",
      "metadata": {
        "id": "6e09c4d1"
      },
      "source": [
        "## Dataset Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "beabaee1",
      "metadata": {
        "id": "beabaee1"
      },
      "source": [
        "Save the predictions in a CSV file and submit it to Kaggle:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d81d8e01",
      "metadata": {
        "id": "d81d8e01"
      },
      "source": [
        "Here is the result:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83231771",
      "metadata": {
        "id": "83231771"
      },
      "source": [
        "We end up with four prediction DataFrames that we will sum and average (this is the so-called ensemble method):"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c348007c",
      "metadata": {
        "id": "c348007c"
      },
      "source": [
        "For each of these parameters, we will train 33 models, run the predictions and fill the final DataFrame. The 3 DataFrames obtained will be stored in the **submission_kaggle_list**:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24340b11",
      "metadata": {
        "id": "24340b11"
      },
      "source": [
        "Train several models and apply the Ensemble method.\n",
        "\n",
        "### Multiple models\n",
        "As explained before, the important thing in this code is the hyperparameters. We will train 3 models by taking the following hyperparameters:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a43e1720",
      "metadata": {
        "id": "a43e1720"
      },
      "source": [
        "Display the predictions:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b595553",
      "metadata": {
        "id": "1b595553"
      },
      "source": [
        "Finally, here is the code that allows to go from the predicted time series cluster to the prediction DataFrame:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03bf785b",
      "metadata": {
        "id": "03bf785b"
      },
      "source": [
        "Note: even if the model has an **output_chunk_length** of 1, we can directly instruct it to predict 16 values in the future.\n",
        "\n",
        "We now have our predictions. If you follow well, you know the next step.\n",
        "\n",
        "Previously, we normalized our data with the **Scaler** function. So the predicted data are also normalized.\n",
        "\n",
        "To de-normalize them we use the **inverse_transform** function on each **TimeSeries**:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8acfb5da",
      "metadata": {
        "id": "8acfb5da"
      },
      "source": [
        "### Predict\n",
        "We can now perform the predictions:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f12f218",
      "metadata": {
        "id": "2f12f218"
      },
      "source": [
        "In the above code, we only use lags_past_covariates = [-16,-17,-18,-19,-20,-21,-22].\n",
        "\n",
        "Because during the 16th prediction (the one of August 31, 2017), the values of the past covariates from -1 to -15 are not known.\n",
        "\n",
        "After training, we obtain 33 Machine Learning models stored in LGBM_Models_Submission."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9356017e",
      "metadata": {
        "id": "9356017e"
      },
      "source": [
        "## Machine Learning Model\n",
        "Now, we will train a first Machine Learning model with the darts library to confirm that our data is consistent and that the predictions obtained are convincing.\n",
        "\n",
        "Then we will use ensemble methods to improve our final result.\n",
        "\n",
        "### Single model\n",
        "The Darts library offers us various Machine Learning models to use on **TimeSeries**.\n",
        "\n",
        "In Ferdinand Berr’s solution, we can see that he uses different models:\n",
        "\n",
        "* **NHiTSModel** – score : 0.43265\n",
        "* **RNNModel** (with LSTM layers) – score : 0.55443\n",
        "* **TFTModel** – score : 0.43226\n",
        "* **ExponentialSmoothing** – score : 0.37411\n",
        "These scores are obtained on validation data, artificially generated from the training data.\n",
        "\n",
        "Personally, I decided to use the LightGBMModel, an implementation of the eponymous library model on which you will find an article here.\n",
        "\n",
        "Why use this model ? Not after hours of practice and experimentation, but simply by using it and seeing that, alone, it gives me better results than the **ExponentialSmoothing**.\n",
        "\n",
        "As explained in the Strategy section, we will train a Machine Learning model for each product family.\n",
        "\n",
        "So for each family, we have to take the corresponding **TimeSeries** and send them to our Machine Learning model.\n",
        "\n",
        "First, we prepare the data:\n",
        "\n",
        "* **TCN_covariates** represents the future covariates associated with the target product family\n",
        "* **train_sliced** represents the number of sales associated with the target product family. The **slice_intersect** function that you can see used simply ensures that the components span the same time interval. In the case of different time intervals an error message will appear if we try to combine them.\n",
        "* **transactions_transformed**, the past covariates do not need to be indexed on the target family because there is only one global **TimeSeries** per store\n",
        "Next, we initialize hyperparameters for our model.\n",
        "\n",
        "**This is the key to model results**.\n",
        "\n",
        "By modifying these hyperparameters you can improve the performance of the Machine Learning model.\n",
        "\n",
        "#### Training\n",
        "Here are the important hyperparameters:\n",
        "\n",
        "* **lags** – the number of past values on which we base our predictions\n",
        "* **lags_future_covariates** – the number of future covariate values on which we base our predictions. If we give a tuple, the left value represents the number of covariates in the past and the right value represents the number of covariates in the future\n",
        "* **lags_past_covariates** – the number of past covariate values on which we base our predictions\n",
        "For these three hyperparameters, if a list is passed, we take the indexes associated with the numbers of this list. For example if we pass: [-3, -4, -5], we take the indexes t-3, t-4, t-5. But if we pass an integer for example 10, we take the 10 previous values (or the 10 future values depending on the case).\n",
        "\n",
        "The hyperparameters **output_chunk_length** controls the number of predicted values in the future, **random_state** ensures the reproducibility of the results and **gpu_use_dp** indicates if we want to use a GPU.\n",
        "\n",
        "After that we launch the training. And at the end, we save the trained model in a dictionary."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "489fbf21",
      "metadata": {
        "id": "489fbf21"
      },
      "source": [
        "Finally ready to create our Machine Learning model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59670e64",
      "metadata": {
        "id": "59670e64"
      },
      "source": [
        "Here is the **TimeSeries** for the first store:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0637a2ef",
      "metadata": {
        "id": "0637a2ef"
      },
      "source": [
        "### Transactions – Past Covariates\n",
        "Before launching the training of the model, let’s extract the past covariates: the transactions.\n",
        "\n",
        "As you might already have understand, after having taken the transactions for each store, we will normalize them:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3c60027",
      "metadata": {
        "id": "a3c60027"
      },
      "source": [
        "Here are the different columns obtained for each **TimeSeries** of each family of each store:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd0ca752",
      "metadata": {
        "id": "bd0ca752"
      },
      "source": [
        "Finally, for each family, we combine the previously created covariates with the promotion covariates:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e32f6b8",
      "metadata": {
        "id": "5e32f6b8"
      },
      "source": [
        "Then for each store, we gather the **TimeSeries** of the holidays with the **general_covariates**:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71cc77ce",
      "metadata": {
        "id": "71cc77ce"
      },
      "source": [
        "#### Grouping the covariates\n",
        "To finish with the future covariates, we are going to gather them in the same **TimeSeries**.\n",
        "\n",
        "We start with the time series of the dates, the oil price and the moving averages of the oil price that we group in the variable **general_covariates**:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a09f61c6",
      "metadata": {
        "id": "a09f61c6"
      },
      "source": [
        "We obtain a normalized time series with 3 columns.\n",
        "\n",
        "We can display the index 1 of the first TimeSeries of the first family:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6b68d6e",
      "metadata": {
        "id": "d6b68d6e"
      },
      "source": [
        "Let’s go further by calculating also the moving average in 7 and 28, like for the oil price:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a21f3190",
      "metadata": {
        "id": "a21f3190"
      },
      "source": [
        "We can display the first **TimeSeries** of the first family :"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d034344f",
      "metadata": {
        "id": "d034344f"
      },
      "source": [
        "#### Promotion\n",
        "The last future covariate to process is the **onpromotion** column.\n",
        "\n",
        "It gives us the number of items on promotion in a product family.\n",
        "\n",
        "Here the code is similar to the one used for the **sales** column. It allows to extract for each family, the time series of the 54 stores:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97244bcc",
      "metadata": {
        "id": "97244bcc"
      },
      "source": [
        "Here is the **TimeSeries** index **100** for the first store:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e66d73f1",
      "metadata": {
        "id": "e66d73f1"
      },
      "source": [
        "#### We get 54 TimeSeries with 7 columns:\n",
        "\n",
        "* national_holiday\n",
        "* earthquake_relief\n",
        "* christmas\n",
        "* football_event\n",
        "* national_event\n",
        "* work_day\n",
        "* local_holiday"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4976f98",
      "metadata": {
        "id": "b4976f98"
      },
      "source": [
        "\n",
        "And finally a function that allows us to have the holidays associated to each of the 54 stores :\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f84a4d5c",
      "metadata": {
        "id": "f84a4d5c"
      },
      "source": [
        "Then, we have a function to remove the days equal to 0 and the duplicates:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d8eaa7b",
      "metadata": {
        "id": "0d8eaa7b"
      },
      "source": [
        "#### Holidays\n",
        "Here, Ferdinand Berr has implemented functions to detail these holidays. In particular, he adds information about whether the holiday is Christmas day, whether it is a soccer game day, etc:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d23b47c",
      "metadata": {
        "id": "2d23b47c"
      },
      "source": [
        "Here is the result obtained at index 100:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4c52494",
      "metadata": {
        "id": "d4c52494"
      },
      "source": [
        "#### Oil\n",
        "As said before, the price of oil is a future covariate because it is known in advance.\n",
        "\n",
        "Here, we will not simply extract the daily oil price but we will calculate the moving average.\n",
        "\n",
        "The **moving average in X**, is an average of the current value and the X-1 previous values of a time series.\n",
        "\n",
        "For example the moving average in 7 is the average of (t + t-1 + … + t-6) / 7. It is calculated at each t, that’s why it is called “moving”.\n",
        "\n",
        "**Calculating the moving average allows us to remove the momentary fluctuations of a value and thus to accentuate the long-term trends**.\n",
        "\n",
        "The moving average is used in trading, but more generally in Time Series Analysis.\n",
        "\n",
        "In the following code, we calculate the moving average in 7 and 28 of the oil price. And of course, we apply a normalization :"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a514ab81",
      "metadata": {
        "id": "a514ab81"
      },
      "source": [
        "You can also see that a split is made between the dates before August 15, 2017 and after (dates that will be used in the prediction)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "450e6c1f",
      "metadata": {
        "id": "450e6c1f"
      },
      "source": [
        "And of course, we will normalize this data:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1d9ce95",
      "metadata": {
        "id": "d1d9ce95"
      },
      "source": [
        "This is what it gives us for the date at index 100:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b00e1fd5",
      "metadata": {
        "id": "b00e1fd5"
      },
      "source": [
        "#### Date\n",
        "The first covariate we are interested in is the date.\n",
        "\n",
        "The date is a future covariate because we know the date of the coming days.\n",
        "\n",
        "It has, in many cases, an impact on the traffic of a store. For example, we can expect that on Saturday there will be more customers in the store than on Monday.\n",
        "\n",
        "But it can also be expected that during the summer vacations the store will be less busy than in normal times.\n",
        "\n",
        "** Hence every little detail counts**.\n",
        "\n",
        "In order not to miss anything, we will extract as much information as possible from this date. Here, 7 columns :\n",
        "\n",
        "* year – year\n",
        "* month – month\n",
        "* day – day\n",
        "* dayofyear – day of the year (for example February 1 is the 32nd day of the year)\n",
        "* weekday – day of the week (there are 7 days in a week)\n",
        "* weekofyear – week of the year (there are 52 weeks in a year)\n",
        "* linear_increase – the index of the interval"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59883715",
      "metadata": {
        "id": "59883715"
      },
      "source": [
        "#### Covariates\n",
        "A covariate is a variable that helps to predict a target variable.\n",
        "\n",
        "This covariate can be dependent on the target variable. For example, the type of store, **type**, where the sales are made. But it can also be independent. For example, the price of oil on the day of the sale of a product.\n",
        "\n",
        "This covariate can be known in advance, for example in our dataset we have the price of oil from January 1, 2013 to August 31, 2017. In this case, we talk about a **future covariate**.\n",
        "\n",
        "There are also **past covariates**. These are covariates that are not known in advance. For example in our dataset, the transactions are known for the dates January 1, 2013 to August 15, 2017."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edaf7eb4",
      "metadata": {
        "id": "edaf7eb4"
      },
      "source": [
        "You can see that the sales have been normalized and that the **static_covariates** have been one hot encoded.\n",
        "\n",
        "We now have our main time series that will allow us to train our model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f092ce2",
      "metadata": {
        "id": "9f092ce2"
      },
      "source": [
        "We can display the first transformed **TimeSeries** of the first family:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d26168a",
      "metadata": {
        "id": "1d26168a"
      },
      "source": [
        "#### Normalizing time series\n",
        "Normalization is a technique used to improve the performance of a Machine Learning model by facilitating its training. I let you refer to our article on the subject if you want to know more.\n",
        "\n",
        "We can easily normalize a **TimeSeries** with the **Scaler** function of darts.\n",
        "\n",
        "Moreover, we will further optimize the training of the model by one hot encoding our covariates. We implement the one hot encoding via the **StaticCovariatesTransformer** function."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcba69cd",
      "metadata": {
        "id": "fcba69cd"
      },
      "source": [
        "We retrieve all the values indicated above: the number of sales, the date of each sale in **Coordinates > date**, and the dependent covariates in **Attributes > static_covariates**.\n",
        "\n",
        "You can also see that the length of the time series is 1688. Originally it was 1684 but we added the values of the four December 25s that are missing from the dataset.\n",
        "\n",
        "Then we apply a normalization to our **TimeSeries**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dd31ffd",
      "metadata": {
        "id": "2dd31ffd"
      },
      "source": [
        "You can also see that we indicate fill_missing_dates=True because in the dataset, the sales of each December 25th are missing.\n",
        "\n",
        "We also indicate freq='D', to indicate that the interval for the values of the time series is in days (D for day).\n",
        "\n",
        "Finally, we indicate that the values of the TimeSeries must be interpreted in float32 and that the time series must be sorted by stores.\n",
        "\n",
        "We can display the first time series of the first family:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f501c11",
      "metadata": {
        "id": "4f501c11"
      },
      "source": [
        "### sales\n",
        "**Extract the time series**\n",
        "\n",
        "For each product family, we will gather all the time series concerning it.\n",
        "\n",
        "So we will have 33 sub-datasets. These datasets will be contained in the family_TS_dict dictionary.\n",
        "\n",
        "In the following lines of code, we extract the TimeSeries of the 54 stores for each family.\n",
        "\n",
        "These TimeSeries will group the sales by family, the date of each sale, but also the dependent covariates (indicated with group_cols and static_cols) of these sales: store_nbr, family, city, state, type, cluster :"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f70aefa",
      "metadata": {
        "id": "9f70aefa"
      },
      "source": [
        "## Main Time Series"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a11514b",
      "metadata": {
        "id": "0a11514b"
      },
      "source": [
        "A time series being the number of sales made per day for a family of a store, this sorting will allow us to extract them more easily.\n",
        "\n",
        "Same thing for the test DataFrame, we sort the sales by date, by family and by stores:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cf04bdf",
      "metadata": {
        "id": "7cf04bdf"
      },
      "source": [
        "Next, we assemble the df_train and df_stores datasets. By grouping this data in a single dataset, it will allow us to access the information more easily. In addition to that, we sort the sales of the DataFrame by date, by family and by stores:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "961935f6",
      "metadata": {
        "id": "961935f6"
      },
      "source": [
        "## Preprocessing\n",
        "To start the preprocessing, let's group the name of each product family and the number of each store:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba6a9482",
      "metadata": {
        "id": "ba6a9482"
      },
      "source": [
        "### The test.csv contains 5 columns:\n",
        "\n",
        "* id – the index of the row (which will be used to fill in the sample_submission.csv file) date – the current date\n",
        "* store_nbr – the store\n",
        "* family – the product family\n",
        "* sales – the number of sales in this family\n",
        "* onpromotion – the number of products on promotion in this family"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c89680c5",
      "metadata": {
        "id": "c89680c5"
      },
      "source": [
        "## test.csv\n",
        "Finally, we have the test.csv that will allow us to predict the sale column. The file starts on August 16, 2017 and ends on August 31, 2017. We also have the sample_submission.csv to fill in with the number of sales per day and per family:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62f757d1",
      "metadata": {
        "id": "62f757d1"
      },
      "source": [
        "## transactions.csv\n",
        "The transactions.csv file groups the daily transactions by stores:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a565fb7",
      "metadata": {
        "id": "0a565fb7"
      },
      "source": [
        "### DataFrame columns:\n",
        "\n",
        "* store_nbr – the store\n",
        "* city – the city where the store is located\n",
        "* state – the state where the store is located\n",
        "* type – the type of the store\n",
        "* cluster – the number of similar stores in the vicini"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4da6b301",
      "metadata": {
        "id": "4da6b301"
      },
      "source": [
        "## store.csv\n",
        "The store.csv file gathers information about the stores. There is one store per line so 54 lines:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5eb312d5",
      "metadata": {
        "id": "5eb312d5"
      },
      "source": [
        "## oil.csv\n",
        "Then a CSV file gathers the daily oil price from January 01, 2013 to August 31, 2017:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2febc12b",
      "metadata": {
        "id": "2febc12b"
      },
      "source": [
        "### Here are the columns in the DataFrame:\n",
        "\n",
        "* date – the date of the holiday\n",
        "* type – the type of holiday (Holiday, Event, Transfer (see transferred column), Additional, Bridge, Work Day)\n",
        "* locale – the scope of the event (Local, Regional, National)\n",
        "* locale_name – the city where the event takes place\n",
        "* description – name of the event\n",
        "* transferred – whether the event has been transferred (moved to another day) or not"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "781a66d8",
      "metadata": {
        "id": "781a66d8"
      },
      "source": [
        "## holidays_events.csv\n",
        "The holidays_events.csv groups the national holidays. This information is independent of the store but can have an impact on sales.\n",
        "\n",
        "For example, on a holiday, there might be more people in the city and therefore more customers in the stores. Or conversely, more people might go on vacation and therefore there would be fewer customers in the stores."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9143f7a8",
      "metadata": {
        "id": "9143f7a8"
      },
      "source": [
        "### Here are the columns of the DataFrame:\n",
        "\n",
        "* id – the index of the row\n",
        "* date – the current date\n",
        "* store_nbr – the store\n",
        "* family – the product family\n",
        "* sales – number of sales in this family\n",
        "* onpromotion – the number of products on promotion in this family"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c16546fd",
      "metadata": {
        "id": "c16546fd"
      },
      "source": [
        "## train.csv\n",
        "First of all the main file: train.csv. It contains some features and the label to predict sales, the number of sales per day:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d7ac4ec",
      "metadata": {
        "id": "0d7ac4ec"
      },
      "source": [
        "# Data\n",
        "First of all I propose to explore the dataset.\n",
        "\n",
        "Our goal is to predict future sales of stores located in Ecuador, for the dates August 16, 2017 to August 31, 2017 (16 days).\n",
        "\n",
        "In our dataset, there are 54 stores for 33 product families.\n",
        "\n",
        "We need to predict the sales for each of these product families from each store. So 33 * 54 * 16 = 28,512 values to predict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a57ad64",
      "metadata": {
        "id": "1a57ad64"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "display(data_train.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6096726",
      "metadata": {
        "id": "f6096726"
      },
      "outputs": [],
      "source": [
        "display(data_holidays.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "442f9eeb",
      "metadata": {
        "id": "442f9eeb"
      },
      "outputs": [],
      "source": [
        "display(data_oil.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3f1ab4d",
      "metadata": {
        "id": "c3f1ab4d"
      },
      "outputs": [],
      "source": [
        "display(data_stores.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d2d99b8",
      "metadata": {
        "id": "9d2d99b8"
      },
      "outputs": [],
      "source": [
        "# Transaction is a receipt created after a customer's purchase\n",
        "display(data_transactions.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ff3c234",
      "metadata": {
        "id": "0ff3c234"
      },
      "outputs": [],
      "source": [
        "display(data_test.head())\n",
        "display(data_sample_submission.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8db7e6a",
      "metadata": {
        "id": "c8db7e6a"
      },
      "outputs": [],
      "source": [
        "family_list = data_train['family'].unique()\n",
        "store_list = data_stores['store_nbr'].unique()\n",
        "display(family_list)\n",
        "display(store_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50e94294",
      "metadata": {
        "id": "50e94294"
      },
      "source": [
        "## Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e13cd8b1",
      "metadata": {
        "id": "e13cd8b1"
      },
      "outputs": [],
      "source": [
        "train_merged = pd.merge(data_train, data_stores, on ='store_nbr')\n",
        "train_merged = train_merged.sort_values([\"store_nbr\",\"family\",\"date\"])\n",
        "train_merged = train_merged.astype({\"store_nbr\":'str', \"family\":'str', \"city\":'str',\n",
        "                          \"state\":'str', \"type\":'str', \"cluster\":'str'})\n",
        "\n",
        "display(train_merged.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4695d00b",
      "metadata": {
        "id": "4695d00b"
      },
      "outputs": [],
      "source": [
        "df_test_dropped = data_test.drop(['onpromotion'], axis=1)\n",
        "df_test_sorted = df_test_dropped.sort_values(by=['store_nbr','family'])\n",
        "\n",
        "display(df_test_sorted.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01948691",
      "metadata": {
        "id": "01948691"
      },
      "outputs": [],
      "source": [
        "!pip install darts==0.23.1\n",
        "#&> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e773b8b",
      "metadata": {
        "id": "8e773b8b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import darts\n",
        "from darts import TimeSeries\n",
        "\n",
        "family_TS_dict = {}\n",
        "\n",
        "for family in family_list:\n",
        "  df_family = train_merged.loc[train_merged['family'] == family]\n",
        "\n",
        "  list_of_TS_family = TimeSeries.from_group_dataframe(\n",
        "                                df_family,\n",
        "                                time_col=\"date\",\n",
        "                                group_cols=[\"store_nbr\",\"family\"],\n",
        "                                static_cols=[\"city\",\"state\",\"type\",\"cluster\"],\n",
        "                                value_cols=\"sales\",\n",
        "                                fill_missing_dates=True,\n",
        "                                freq='D')\n",
        "  for ts in list_of_TS_family:\n",
        "            ts = ts.astype(np.float32)\n",
        "\n",
        "  list_of_TS_family = sorted(list_of_TS_family, key=lambda ts: int(ts.static_covariates_values()[0,0]))\n",
        "  family_TS_dict[family] = list_of_TS_family"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42ccc42b",
      "metadata": {
        "id": "42ccc42b"
      },
      "outputs": [],
      "source": [
        "display(family_TS_dict['AUTOMOTIVE'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e077e966",
      "metadata": {
        "id": "e077e966"
      },
      "outputs": [],
      "source": [
        "from darts.dataprocessing import Pipeline\n",
        "from darts.dataprocessing.transformers import Scaler, StaticCovariatesTransformer, MissingValuesFiller, InvertibleMapper\n",
        "import sklearn\n",
        "\n",
        "family_pipeline_dict = {}\n",
        "family_TS_transformed_dict = {}\n",
        "\n",
        "for key in family_TS_dict:\n",
        "  train_filler = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Fill NAs\")\n",
        "  static_cov_transformer = StaticCovariatesTransformer(verbose=False, transformer_cat = sklearn.preprocessing.OneHotEncoder(), name=\"Encoder\")\n",
        "  log_transformer = InvertibleMapper(np.log1p, np.expm1, verbose=False, n_jobs=-1, name=\"Log-Transform\")\n",
        "  train_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaling\")\n",
        "\n",
        "  train_pipeline = Pipeline([train_filler,\n",
        "                             static_cov_transformer,\n",
        "                             log_transformer,\n",
        "                             train_scaler])\n",
        "\n",
        "  training_transformed = train_pipeline.fit_transform(family_TS_dict[key])\n",
        "  family_pipeline_dict[key] = train_pipeline\n",
        "  family_TS_transformed_dict[key] = training_transformed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "838f6908",
      "metadata": {
        "id": "838f6908"
      },
      "outputs": [],
      "source": [
        "display(family_TS_transformed_dict['AUTOMOTIVE'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42bbc53f",
      "metadata": {
        "id": "42bbc53f"
      },
      "outputs": [],
      "source": [
        "from darts.utils.timeseries_generation import datetime_attribute_timeseries\n",
        "\n",
        "full_time_period = pd.date_range(start='2013-01-01', end='2017-08-31', freq='D')\n",
        "\n",
        "\n",
        "year = datetime_attribute_timeseries(time_index = full_time_period, attribute=\"year\")\n",
        "month = datetime_attribute_timeseries(time_index = full_time_period, attribute=\"month\")\n",
        "day = datetime_attribute_timeseries(time_index = full_time_period, attribute=\"day\")\n",
        "dayofyear = datetime_attribute_timeseries(time_index = full_time_period, attribute=\"dayofyear\")\n",
        "weekday = datetime_attribute_timeseries(time_index = full_time_period, attribute=\"dayofweek\")\n",
        "weekofyear = datetime_attribute_timeseries(time_index = full_time_period, attribute=\"weekofyear\")\n",
        "timesteps = TimeSeries.from_times_and_values(times=full_time_period,\n",
        "                                             values=np.arange(len(full_time_period)),\n",
        "                                             columns=[\"linear_increase\"])\n",
        "\n",
        "time_cov = year.stack(month).stack(day).stack(dayofyear).stack(weekday).stack(weekofyear).stack(timesteps)\n",
        "time_cov = time_cov.astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4856068e",
      "metadata": {
        "id": "4856068e"
      },
      "outputs": [],
      "source": [
        "display(print(time_cov.components.values))\n",
        "display(time_cov[100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bd833a9",
      "metadata": {
        "id": "0bd833a9"
      },
      "outputs": [],
      "source": [
        "time_cov_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaler\")\n",
        "time_cov_train, time_cov_val = time_cov.split_before(pd.Timestamp('20170816'))\n",
        "time_cov_scaler.fit(time_cov_train)\n",
        "time_cov_transformed = time_cov_scaler.transform(time_cov)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a3f3a52",
      "metadata": {
        "id": "2a3f3a52"
      },
      "source": [
        "## Feature Engineering & Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3cbc20f",
      "metadata": {
        "id": "e3cbc20f"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade scikit-learn==1.2.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d13b383d",
      "metadata": {
        "id": "d13b383d"
      },
      "outputs": [],
      "source": [
        "from darts.models import MovingAverage\n",
        "\n",
        "# Oil Price\n",
        "\n",
        "oil = TimeSeries.from_dataframe(data_oil,\n",
        "                                time_col = 'date',\n",
        "                                value_cols = ['dcoilwtico'],\n",
        "                                freq = 'D')\n",
        "\n",
        "oil = oil.astype(np.float32)\n",
        "\n",
        "# Transform\n",
        "oil_filler = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Filler\")\n",
        "oil_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaler\")\n",
        "oil_pipeline = Pipeline([oil_filler, oil_scaler])\n",
        "oil_transformed = oil_pipeline.fit_transform(oil)\n",
        "\n",
        "# Moving Averages for Oil Price\n",
        "oil_moving_average_7 = MovingAverage(window=7)\n",
        "oil_moving_average_28 = MovingAverage(window=28)\n",
        "\n",
        "oil_moving_averages = []\n",
        "\n",
        "ma_7 = oil_moving_average_7.filter(oil_transformed).astype(np.float32)\n",
        "ma_7 = ma_7.with_columns_renamed(col_names=ma_7.components, col_names_new=\"oil_ma_7\")\n",
        "ma_28 = oil_moving_average_28.filter(oil_transformed).astype(np.float32)\n",
        "ma_28 = ma_28.with_columns_renamed(col_names=ma_28.components, col_names_new=\"oil_ma_28\")\n",
        "oil_moving_averages = ma_7.stack(ma_28)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ff2c76d",
      "metadata": {
        "id": "7ff2c76d"
      },
      "outputs": [],
      "source": [
        "display(oil_moving_averages[100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa6b07ef",
      "metadata": {
        "id": "aa6b07ef"
      },
      "outputs": [],
      "source": [
        "def holiday_list(data_stores):\n",
        "\n",
        "    listofseries = []\n",
        "\n",
        "    for i in range(0,len(data_stores)):\n",
        "\n",
        "            df_holiday_dummies = pd.DataFrame(columns=['date'])\n",
        "            df_holiday_dummies[\"date\"] = data_holidays[\"date\"]\n",
        "\n",
        "            df_holiday_dummies[\"national_holiday\"] = np.where(((data_holidays[\"type\"] == \"Holiday\") & (data_holidays[\"locale\"] == \"National\")), 1, 0)\n",
        "\n",
        "            df_holiday_dummies[\"earthquake_relief\"] = np.where(data_holidays['description'].str.contains('Terremoto Manabi'), 1, 0)\n",
        "\n",
        "            df_holiday_dummies[\"christmas\"] = np.where(data_holidays['description'].str.contains('Navidad'), 1, 0)\n",
        "\n",
        "            df_holiday_dummies[\"football_event\"] = np.where(data_holidays['description'].str.contains('futbol'), 1, 0)\n",
        "\n",
        "            df_holiday_dummies[\"national_event\"] = np.where(((data_holidays[\"type\"] == \"Event\") & (data_holidays[\"locale\"] == \"National\") & (~data_holidays['description'].str.contains('Terremoto Manabi')) & (~data_holidays['description'].str.contains('futbol'))), 1, 0)\n",
        "\n",
        "            df_holiday_dummies[\"work_day\"] = np.where((data_holidays[\"type\"] == \"Work Day\"), 1, 0)\n",
        "\n",
        "            df_holiday_dummies[\"local_holiday\"] = np.where(((data_holidays[\"type\"] == \"Holiday\") & ((data_holidays[\"locale_name\"] == data_stores['state'][i]) | (data_holidays[\"locale_name\"] == data_stores['city'][i]))), 1, 0)\n",
        "\n",
        "            listofseries.append(df_holiday_dummies)\n",
        "\n",
        "    return listofseries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b9e85e6",
      "metadata": {
        "id": "5b9e85e6"
      },
      "outputs": [],
      "source": [
        "def remove_0_and_duplicates(holiday_list):\n",
        "\n",
        "    listofseries = []\n",
        "\n",
        "    for i in range(0,len(holiday_list)):\n",
        "\n",
        "            df_holiday_per_store = list_of_holidays_per_store[i].set_index('date')\n",
        "\n",
        "            df_holiday_per_store = df_holiday_per_store.loc[~(df_holiday_per_store==0).all(axis=1)]\n",
        "\n",
        "            df_holiday_per_store = df_holiday_per_store.groupby('date').agg({'national_holiday':'max', 'earthquake_relief':'max',\n",
        "                                   'christmas':'max', 'football_event':'max',\n",
        "                                   'national_event':'max', 'work_day':'max',\n",
        "                                   'local_holiday':'max'}).reset_index()\n",
        "\n",
        "            listofseries.append(df_holiday_per_store)\n",
        "\n",
        "    return listofseries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c63cb8c",
      "metadata": {
        "id": "5c63cb8c"
      },
      "outputs": [],
      "source": [
        "def holiday_TS_list_54(holiday_list):\n",
        "\n",
        "    listofseries = []\n",
        "\n",
        "    for i in range(0,54):\n",
        "\n",
        "            holidays_TS = TimeSeries.from_dataframe(list_of_holidays_per_store[i],\n",
        "                                        time_col = 'date',\n",
        "                                        fill_missing_dates=True,\n",
        "                                        fillna_value=0,\n",
        "                                        freq='D')\n",
        "\n",
        "            holidays_TS = holidays_TS.slice(pd.Timestamp('20130101'),pd.Timestamp('20170831'))\n",
        "            holidays_TS = holidays_TS.astype(np.float32)\n",
        "            listofseries.append(holidays_TS)\n",
        "\n",
        "    return listofseries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4835334",
      "metadata": {
        "id": "c4835334"
      },
      "outputs": [],
      "source": [
        "list_of_holidays_per_store = holiday_list(data_stores)\n",
        "list_of_holidays_per_store = remove_0_and_duplicates(list_of_holidays_per_store)\n",
        "list_of_holidays_store = holiday_TS_list_54(list_of_holidays_per_store)\n",
        "\n",
        "holidays_filler = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Filler\")\n",
        "holidays_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaler\")\n",
        "\n",
        "holidays_pipeline = Pipeline([holidays_filler, holidays_scaler])\n",
        "holidays_transformed = holidays_pipeline.fit_transform(list_of_holidays_store)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1eceb76",
      "metadata": {
        "id": "c1eceb76"
      },
      "outputs": [],
      "source": [
        "display(len(holidays_transformed))\n",
        "display(holidays_transformed[0].components.values)\n",
        "display(holidays_transformed[0][100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc35fbf6",
      "metadata": {
        "id": "cc35fbf6"
      },
      "outputs": [],
      "source": [
        "df_promotion = pd.concat([data_train, data_test], axis=0)\n",
        "df_promotion = df_promotion.sort_values([\"store_nbr\",\"family\",\"date\"])\n",
        "df_promotion.tail()\n",
        "\n",
        "family_promotion_dict = {}\n",
        "\n",
        "for family in family_list:\n",
        "  df_family = df_promotion.loc[df_promotion['family'] == family]\n",
        "\n",
        "  list_of_TS_promo = TimeSeries.from_group_dataframe(\n",
        "                                df_family,\n",
        "                                time_col=\"date\",\n",
        "                                group_cols=[\"store_nbr\",\"family\"],\n",
        "                                value_cols=\"onpromotion\",\n",
        "                                fill_missing_dates=True,\n",
        "                                freq='D')\n",
        "\n",
        "  for ts in list_of_TS_promo:\n",
        "    ts = ts.astype(np.float32)\n",
        "\n",
        "  family_promotion_dict[family] = list_of_TS_promo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0906b574",
      "metadata": {
        "id": "0906b574"
      },
      "outputs": [],
      "source": [
        "display(family_promotion_dict['AUTOMOTIVE'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a61be26",
      "metadata": {
        "id": "0a61be26"
      },
      "source": [
        "## Modeling & Forecasting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a291108d",
      "metadata": {
        "id": "a291108d"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "promotion_transformed_dict = {}\n",
        "\n",
        "for key in tqdm(family_promotion_dict):\n",
        "  promo_filler = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Fill NAs\")\n",
        "  promo_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaling\")\n",
        "\n",
        "  promo_pipeline = Pipeline([promo_filler,\n",
        "                             promo_scaler])\n",
        "\n",
        "  promotion_transformed = promo_pipeline.fit_transform(family_promotion_dict[key])\n",
        "\n",
        "  # Moving Averages for Promotion Family Dictionaries\n",
        "  promo_moving_average_7 = MovingAverage(window=7)\n",
        "  promo_moving_average_28 = MovingAverage(window=28)\n",
        "\n",
        "  promotion_covs = []\n",
        "\n",
        "  for ts in promotion_transformed:\n",
        "    ma_7 = promo_moving_average_7.filter(ts)\n",
        "    ma_7 = TimeSeries.from_series(ma_7.pd_series())\n",
        "    ma_7 = ma_7.astype(np.float32)\n",
        "    ma_7 = ma_7.with_columns_renamed(col_names=ma_7.components, col_names_new=\"promotion_ma_7\")\n",
        "    ma_28 = promo_moving_average_28.filter(ts)\n",
        "    ma_28 = TimeSeries.from_series(ma_28.pd_series())\n",
        "    ma_28 = ma_28.astype(np.float32)\n",
        "    ma_28 = ma_28.with_columns_renamed(col_names=ma_28.components, col_names_new=\"promotion_ma_28\")\n",
        "    promo_and_mas = ts.stack(ma_7).stack(ma_28)\n",
        "    promotion_covs.append(promo_and_mas)\n",
        "\n",
        "  promotion_transformed_dict[key] = promotion_covs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af765ce3",
      "metadata": {
        "id": "af765ce3"
      },
      "outputs": [],
      "source": [
        "display(promotion_transformed_dict['AUTOMOTIVE'][0].components.values)\n",
        "display(promotion_transformed_dict['AUTOMOTIVE'][0][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74c87702",
      "metadata": {
        "id": "74c87702"
      },
      "outputs": [],
      "source": [
        "general_covariates = time_cov_transformed.stack(oil_transformed).stack(oil_moving_averages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1ab4876",
      "metadata": {
        "id": "a1ab4876"
      },
      "outputs": [],
      "source": [
        "store_covariates_future = []\n",
        "\n",
        "for store in range(0,len(store_list)):\n",
        "  stacked_covariates = holidays_transformed[store].stack(general_covariates)\n",
        "  store_covariates_future.append(stacked_covariates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0da74d19",
      "metadata": {
        "id": "0da74d19"
      },
      "outputs": [],
      "source": [
        "future_covariates_dict = {}\n",
        "\n",
        "for key in tqdm(promotion_transformed_dict):\n",
        "\n",
        "  promotion_family = promotion_transformed_dict[key]\n",
        "  covariates_future = [promotion_family[i].stack(store_covariates_future[i]) for i in range(0,len(promotion_family))]\n",
        "\n",
        "  future_covariates_dict[key] = covariates_future"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f8a8dec",
      "metadata": {
        "id": "3f8a8dec"
      },
      "outputs": [],
      "source": [
        "display(future_covariates_dict['AUTOMOTIVE'][0].components)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7be1176",
      "metadata": {
        "id": "e7be1176"
      },
      "outputs": [],
      "source": [
        "data_transactions.sort_values([\"store_nbr\",\"date\"], inplace=True)\n",
        "\n",
        "TS_transactions_list = TimeSeries.from_group_dataframe(\n",
        "                                data_transactions,\n",
        "                                time_col=\"date\",\n",
        "                                group_cols=[\"store_nbr\"],\n",
        "                                value_cols=\"transactions\",\n",
        "                                fill_missing_dates=True,\n",
        "                                freq='D')\n",
        "\n",
        "transactions_list = []\n",
        "\n",
        "for ts in TS_transactions_list:\n",
        "            series = TimeSeries.from_series(ts.pd_series())\n",
        "            series = series.astype(np.float32)\n",
        "            transactions_list.append(series)\n",
        "\n",
        "transactions_list[24] = transactions_list[24].slice(start_ts=pd.Timestamp('20130102'), end_ts=pd.Timestamp('20170815'))\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "transactions_list_full = []\n",
        "\n",
        "for ts in transactions_list:\n",
        "  if ts.start_time() > pd.Timestamp('20130101'):\n",
        "    end_time = (ts.start_time() - timedelta(days=1))\n",
        "    delta = end_time - pd.Timestamp('20130101')\n",
        "    zero_series = TimeSeries.from_times_and_values(\n",
        "                              times=pd.date_range(start=pd.Timestamp('20130101'),\n",
        "                              end=end_time, freq=\"D\"),\n",
        "                              values=np.zeros(delta.days+1))\n",
        "    ts = zero_series.append(ts)\n",
        "    ts = ts.with_columns_renamed(col_names=ts.components, col_names_new=\"transactions\")\n",
        "    transactions_list_full.append(ts)\n",
        "\n",
        "transactions_filler = MissingValuesFiller(verbose=False, n_jobs=-1, name=\"Filler\")\n",
        "transactions_scaler = Scaler(verbose=False, n_jobs=-1, name=\"Scaler\")\n",
        "\n",
        "transactions_pipeline = Pipeline([transactions_filler, transactions_scaler])\n",
        "transactions_transformed = transactions_pipeline.fit_transform(transactions_list_full)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e88840e",
      "metadata": {
        "id": "5e88840e"
      },
      "outputs": [],
      "source": [
        "display(transactions_transformed[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e927473",
      "metadata": {
        "id": "0e927473"
      },
      "outputs": [],
      "source": [
        "from darts.models import LightGBMModel\n",
        "\n",
        "LGBM_Models_Submission = {}\n",
        "\n",
        "display(\"Training...\")\n",
        "\n",
        "for family in tqdm(family_list):\n",
        "\n",
        "  sales_family = family_TS_transformed_dict[family]\n",
        "  training_data = [ts for ts in sales_family]\n",
        "  TCN_covariates = future_covariates_dict[family]\n",
        "  train_sliced = [training_data[i].slice_intersect(TCN_covariates[i]) for i in range(0,len(training_data))]\n",
        "\n",
        "  LGBM_Model_Submission = LightGBMModel(lags = 63,\n",
        "                                        lags_future_covariates = (14,1),\n",
        "                                        lags_past_covariates = [-16,-17,-18,-19,-20,-21,-22],\n",
        "                                        output_chunk_length=1,\n",
        "                                        random_state=2022,\n",
        "                                        gpu_use_dp= \"false\",\n",
        "                                        )\n",
        "\n",
        "  LGBM_Model_Submission.fit(series=train_sliced,\n",
        "                        future_covariates=TCN_covariates,\n",
        "                        past_covariates=transactions_transformed)\n",
        "\n",
        "  LGBM_Models_Submission[family] = LGBM_Model_Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32986bb7",
      "metadata": {
        "id": "32986bb7"
      },
      "outputs": [],
      "source": [
        "display(\"Predictions...\")\n",
        "\n",
        "LGBM_Forecasts_Families_Submission = {}\n",
        "\n",
        "for family in tqdm(family_list):\n",
        "\n",
        "  sales_family = family_TS_transformed_dict[family]\n",
        "  training_data = [ts for ts in sales_family]\n",
        "  LGBM_covariates = future_covariates_dict[family]\n",
        "  train_sliced = [training_data[i].slice_intersect(TCN_covariates[i]) for i in range(0,len(training_data))]\n",
        "\n",
        "  forecast_LGBM = LGBM_Models_Submission[family].predict(n=16,\n",
        "                                         series=train_sliced,\n",
        "                                         future_covariates=LGBM_covariates,\n",
        "                                         past_covariates=transactions_transformed)\n",
        "\n",
        "  LGBM_Forecasts_Families_Submission[family] = forecast_LGBM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfa517e1",
      "metadata": {
        "id": "dfa517e1"
      },
      "source": [
        "## Evaluation & Error Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1933bf62",
      "metadata": {
        "id": "1933bf62"
      },
      "outputs": [],
      "source": [
        "LGBM_Forecasts_Families_back_Submission = {}\n",
        "\n",
        "for family in tqdm(family_list):\n",
        "\n",
        "  LGBM_Forecasts_Families_back_Submission[family] = family_pipeline_dict[family].inverse_transform(LGBM_Forecasts_Families_Submission[family], partial=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67fdace9",
      "metadata": {
        "id": "67fdace9"
      },
      "outputs": [],
      "source": [
        "for family in tqdm(LGBM_Forecasts_Families_back_Submission):\n",
        "  for n in range(0,len(LGBM_Forecasts_Families_back_Submission[family])):\n",
        "    if (family_TS_dict[family][n].univariate_values()[-21:] == 0).all():\n",
        "        LGBM_Forecasts_Families_back_Submission[family][n] = LGBM_Forecasts_Families_back_Submission[family][n].map(lambda x: x * 0)\n",
        "\n",
        "listofseries = []\n",
        "\n",
        "for store in tqdm(range(0,54)):\n",
        "  for family in family_list:\n",
        "      oneforecast = LGBM_Forecasts_Families_back_Submission[family][store].pd_dataframe()\n",
        "      oneforecast.columns = ['fcast']\n",
        "      listofseries.append(oneforecast)\n",
        "\n",
        "df_forecasts = pd.concat(listofseries)\n",
        "df_forecasts.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# No Negative Forecasts\n",
        "df_forecasts[df_forecasts < 0] = 0\n",
        "forecasts_kaggle = pd.concat([df_test_sorted, df_forecasts.set_index(df_test_sorted.index)], axis=1)\n",
        "forecasts_kaggle_sorted = forecasts_kaggle.sort_values(by=['id'])\n",
        "forecasts_kaggle_sorted = forecasts_kaggle_sorted.drop(['date','store_nbr','family'], axis=1)\n",
        "forecasts_kaggle_sorted = forecasts_kaggle_sorted.rename(columns={\"fcast\": \"sales\"})\n",
        "forecasts_kaggle_sorted = forecasts_kaggle_sorted.reset_index(drop=True)\n",
        "\n",
        "# Submission\n",
        "submission_kaggle = forecasts_kaggle_sorted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "439f9cc8",
      "metadata": {
        "id": "439f9cc8"
      },
      "outputs": [],
      "source": [
        "submission_kaggle.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "484e764b",
      "metadata": {
        "id": "484e764b"
      },
      "outputs": [],
      "source": [
        "model_params = [\n",
        "    {\"lags\" : 7, \"lags_future_covariates\" : (16,1), \"lags_past_covariates\" : [-16,-17,-18,-19,-20,-21,-22]},\n",
        "    {\"lags\" : 365, \"lags_future_covariates\" : (14,1), \"lags_past_covariates\" : [-16,-17,-18,-19,-20,-21,-22]},\n",
        "    {\"lags\" : 730, \"lags_future_covariates\" : (14,1), \"lags_past_covariates\" : [-16,-17,-18,-19,-20,-21,-22]}\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "777db0d0",
      "metadata": {
        "id": "777db0d0"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_log_error as msle, mean_squared_error as mse\n",
        "from lightgbm import early_stopping\n",
        "\n",
        "submission_kaggle_list = []\n",
        "\n",
        "for params in model_params:\n",
        "\n",
        "  LGBM_Models_Submission = {}\n",
        "\n",
        "  display(\"Training...\")\n",
        "\n",
        "  for family in tqdm(family_list):\n",
        "\n",
        "    # Define Data for family\n",
        "    sales_family = family_TS_transformed_dict[family]\n",
        "    training_data = [ts for ts in sales_family]\n",
        "    TCN_covariates = future_covariates_dict[family]\n",
        "    train_sliced = [training_data[i].slice_intersect(TCN_covariates[i]) for i in range(0,len(training_data))]\n",
        "\n",
        "    LGBM_Model_Submission = LightGBMModel(lags = params[\"lags\"],\n",
        "                                          lags_future_covariates = params[\"lags_future_covariates\"],\n",
        "                                          lags_past_covariates = params[\"lags_past_covariates\"],\n",
        "                                          output_chunk_length=1,\n",
        "                                          random_state=2022,\n",
        "                                          gpu_use_dp= \"false\")\n",
        "\n",
        "    LGBM_Model_Submission.fit(series=train_sliced,\n",
        "                          future_covariates=TCN_covariates,\n",
        "                          past_covariates=transactions_transformed)\n",
        "\n",
        "    LGBM_Models_Submission[family] = LGBM_Model_Submission\n",
        "\n",
        "  display(\"Predictions...\")\n",
        "\n",
        "\n",
        "  LGBM_Forecasts_Families_Submission = {}\n",
        "\n",
        "  for family in tqdm(family_list):\n",
        "\n",
        "    sales_family = family_TS_transformed_dict[family]\n",
        "    training_data = [ts for ts in sales_family]\n",
        "    LGBM_covariates = future_covariates_dict[family]\n",
        "    train_sliced = [training_data[i].slice_intersect(TCN_covariates[i]) for i in range(0,len(training_data))]\n",
        "\n",
        "    forecast_LGBM = LGBM_Models_Submission[family].predict(n=16,\n",
        "                                          series=train_sliced,\n",
        "                                          future_covariates=LGBM_covariates,\n",
        "                                          past_covariates=transactions_transformed)\n",
        "\n",
        "    LGBM_Forecasts_Families_Submission[family] = forecast_LGBM\n",
        "\n",
        "  # Transform Back\n",
        "\n",
        "  LGBM_Forecasts_Families_back_Submission = {}\n",
        "\n",
        "  for family in tqdm(family_list):\n",
        "\n",
        "    LGBM_Forecasts_Families_back_Submission[family] = family_pipeline_dict[family].inverse_transform(LGBM_Forecasts_Families_Submission[family], partial=True)\n",
        "\n",
        "  # Prepare Submission in Correct Format\n",
        "\n",
        "  for family in tqdm(LGBM_Forecasts_Families_back_Submission):\n",
        "    for n in range(0,len(LGBM_Forecasts_Families_back_Submission[family])):\n",
        "      if (family_TS_dict[family][n].univariate_values()[-21:] == 0).all():\n",
        "          LGBM_Forecasts_Families_back_Submission[family][n] = LGBM_Forecasts_Families_back_Submission[family][n].map(lambda x: x * 0)\n",
        "\n",
        "  listofseries = []\n",
        "\n",
        "  for store in tqdm(range(0,54)):\n",
        "    for family in family_list:\n",
        "        oneforecast = LGBM_Forecasts_Families_back_Submission[family][store].pd_dataframe()\n",
        "        oneforecast.columns = ['fcast']\n",
        "        listofseries.append(oneforecast)\n",
        "\n",
        "  df_forecasts = pd.concat(listofseries)\n",
        "  df_forecasts.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  # No Negative Forecasts\n",
        "  df_forecasts[df_forecasts < 0] = 0\n",
        "  forecasts_kaggle = pd.concat([df_test_sorted, df_forecasts.set_index(df_test_sorted.index)], axis=1)\n",
        "  forecasts_kaggle_sorted = forecasts_kaggle.sort_values(by=['id'])\n",
        "  forecasts_kaggle_sorted = forecasts_kaggle_sorted.drop(['date','store_nbr','family'], axis=1)\n",
        "  forecasts_kaggle_sorted = forecasts_kaggle_sorted.rename(columns={\"fcast\": \"sales\"})\n",
        "  forecasts_kaggle_sorted = forecasts_kaggle_sorted.reset_index(drop=True)\n",
        "\n",
        "  # Submission\n",
        "  submission_kaggle_list.append(forecasts_kaggle_sorted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "412864ad",
      "metadata": {
        "id": "412864ad"
      },
      "outputs": [],
      "source": [
        "data_sample_submission['sales'] = (submission_kaggle[['sales']]+submission_kaggle_list[0][['sales']]+submission_kaggle_list[1][['sales']]+submission_kaggle_list[2][['sales']])/4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e34eaa9",
      "metadata": {
        "id": "4e34eaa9"
      },
      "outputs": [],
      "source": [
        "data_sample_submission.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ef3d30e",
      "metadata": {
        "id": "3ef3d30e"
      },
      "outputs": [],
      "source": [
        "data_sample_submission.to_csv('/content/drive/MyDrive/DS_Army/submission.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68c84be2",
      "metadata": {
        "id": "68c84be2"
      },
      "source": [
        "\n",
        "## Final Insights & Recommendations\n",
        "\n",
        "- Sales exhibit **strong weekly and seasonal patterns**.\n",
        "- External features like **holidays and oil prices** show noticeable influence on sales spikes and drops.\n",
        "- Using models like **LightGBM** or **XGBoost** yields strong predictive performance with appropriate feature engineering.\n",
        "- Future iterations could enhance performance with:\n",
        "  - Store-specific seasonality modeling\n",
        "  - More granular promotional data\n",
        "  - Deep learning models (e.g., Temporal Fusion Transformer)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}